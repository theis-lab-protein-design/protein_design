{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=5\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-09-03 20:41:14,215] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "from add_linker import convert_fasta\n",
    "from create_empty_msas import main as create_empty_msas\n",
    "from debug_openfold import run_prediction as run_openfold\n",
    "from extract_best_seq import parse_and_extract\n",
    "from pmpnn import main as run_protein_mpnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs/homedirs/hetzell/code/protein_design/example_outputs/03Sep24_dna_design_0.4\n"
     ]
    }
   ],
   "source": [
    "PREFIX = \"dna_design_0.4\"\n",
    "auto_generate = True\n",
    "\n",
    "if not auto_generate:\n",
    "    OUTPUT_DIR = Path(\"/nfs/homedirs/hetzell/code/protein_design/example_outputs\") / f\"30Apr24_looping\"\n",
    "else:\n",
    "    current_date = datetime.now()\n",
    "    ddmmmyy_prefix = current_date.strftime(\"%d%b%y\")\n",
    "\n",
    "    OUTPUT_DIR = Path(\"/nfs/homedirs/hetzell/code/protein_design/example_outputs\") / f\"{ddmmmyy_prefix}_{PREFIX}\"\n",
    "\n",
    "if not OUTPUT_DIR.exists():\n",
    "    OUTPUT_DIR.mkdir(parents=True)\n",
    "\n",
    "print(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C8_designs = Path(\"/nfs/homedirs/hetzell/code/protein_project/protein-backbone-MCTS/outputs/C_small_chunks\")\n",
    "# INPUT_PDB = C8_designs / \"Subfolder_3\" \n",
    "# INPUT_PDB = Path(\"/nfs/homedirs/hetzell/code/protein-frame-flow/inference_outputs/hallucination_pdb/2024-07-25_11-48-32/last/unconditional/run_2024-08-23_12-46-11/pdbs\")\n",
    "INPUT_PDB = Path(\"/nfs/homedirs/hetzell/code/protein_design/example_outputs/03Sep24_dna_design_0.4/input\")\n",
    "OUTPUT_PDB = OUTPUT_DIR/\"pdbs\"\n",
    "# Ensure the output directory exists\n",
    "if not OUTPUT_PDB.exists():\n",
    "    OUTPUT_PDB.mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "# Move all PDB files to OUTPUT_DIR\n",
    "for pdb in INPUT_PDB.iterdir():\n",
    "    if pdb.is_file() and pdb.suffix.lower() == '.pdb':  # Ensure it's a file and has the .pdb extension\n",
    "        target_path = OUTPUT_PDB/ pdb.name\n",
    "        if target_path.exists():\n",
    "            continue\n",
    "        try:\n",
    "            shutil.copy(str(pdb), str(target_path))\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to move {pdb} to {target_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "chain_id_jsonl is NOT loaded\n",
      "----------------------------------------\n",
      "fixed_positions_jsonl is NOT loaded\n",
      "----------------------------------------\n",
      "pssm_jsonl is NOT loaded\n",
      "----------------------------------------\n",
      "omit_AA_jsonl is NOT loaded\n",
      "----------------------------------------\n",
      "bias_AA_jsonl is NOT loaded\n",
      "----------------------------------------\n",
      "tied_positions_jsonl is NOT loaded\n",
      "----------------------------------------\n",
      "bias by residue dictionary is not loaded, or not provided\n",
      "----------------------------------------\n",
      "discarded {'bad_chars': 0, 'too_long': 0, 'bad_seq_length': 0}\n",
      "----------------------------------------\n",
      "Number of edges: 48\n",
      "Training noise level: 0.2A\n",
      "Generating sequences for: template_dna\n",
      "1000 sequences of length 602 generated in 142.185 seconds\n"
     ]
    }
   ],
   "source": [
    "run_protein_mpnn(str(OUTPUT_PDB), str(OUTPUT_DIR/'protein_mpnn'), symmetry=False, seqs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences     1/    1 done.\n"
     ]
    }
   ],
   "source": [
    "FASTA_DIR = OUTPUT_DIR/ \"protein_mpnn\" / \"seqs\"\n",
    "OUTPUT_SEQS = FASTA_DIR.parent / \"seqs_best\"\n",
    "fasta_files = [f for f in FASTA_DIR.iterdir() if f.suffix in [\".fasta\", \".fa\"]]\n",
    "\n",
    "for i, f in enumerate(fasta_files):\n",
    "    parse_and_extract(f, OUTPUT_SEQS, overwrite=True, n_seqs=250, add_monomer=True, add_trimer=False)\n",
    "    print(f\"Sequences {i+1:5}/{len(fasta_files):5} done.\")\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences   300/  300 done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fasta_files = [f for f in (OUTPUT_DIR/ \"protein_mpnn\"/ \"seqs_best\"/\"trimer\").iterdir() if f.suffix in [\".fasta\", \".fa\"]]\n",
    "\n",
    "OUTPUT_LINKER = OUTPUT_DIR/ \"protein_mpnn\"/ \"seqs_best\" / \"trimer_linker\"\n",
    "\n",
    "OUTPUT_LINKER.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for i, f in enumerate(fasta_files):\n",
    "    o = OUTPUT_LINKER / f\"{f.stem}_linker.fasta\"\n",
    "    convert_fasta(f, o)\n",
    "    print(f\"Sequences {i+1:5}/{len(fasta_files):5} done.\\n\")\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting = 'trimer_linker'\n",
    "setting = 'monomer'\n",
    "\n",
    "OUTPUT_OPENFOLD = OUTPUT_DIR / f\"openfold_{setting}\"\n",
    "# FASTA_DIR = OUTPUT_LINKER\n",
    "FASTA_DIR = OUTPUT_DIR/ \"protein_mpnn\"/ \"seqs_best\" / f\"{setting}\"\n",
    "\n",
    "create_empty_msas(str(FASTA_DIR), str(OUTPUT_OPENFOLD / \"alignments\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/nfs/homedirs/hetzell/code/protein_design/example_outputs/24Aug24_frameflow_design/protein_mpnn/seqs_best/homomer')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FASTA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [19], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# # Only test on a subset of sequences\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# _FASTA_DIR = FASTA_DIR.parent / \"trimer_linker_test\"\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mrun_openfold\u001b[49m\u001b[43m(\u001b[49m\n        run_openfold \u001b[0;34m= <function run_prediction at 0x7f795dce8ee0>\u001b[0m\u001b[0;34m\n        \u001b[0mFASTA_DIR \u001b[0;34m= PosixPath('/nfs/homedirs/hetzell/code/protein_design/example_outputs/25Jul24_dna_design_0.2/protein_mpnn/seqs_best/monomer')\u001b[0m\u001b[0;34m\n        \u001b[0mOUTPUT_OPENFOLD \u001b[0;34m= PosixPath('/nfs/homedirs/hetzell/code/protein_design/example_outputs/25Jul24_dna_design_0.2/openfold_monomer')\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfasta_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mFASTA_DIR\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_precomputed_alignments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mOUTPUT_OPENFOLD\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43malignments\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_preset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mOUTPUT_OPENFOLD\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nfs/staff-ssd/hetzell/code/protein_design/debug_openfold.py:85\u001b[0m, in \u001b[0;36mrun_prediction\u001b[0;34m(fasta_dir='/nfs/homedirs/hetzell/code/protein_design/exampl...l24_dna_design_0.2/protein_mpnn/seqs_best/monomer', use_precomputed_alignments='/nfs/homedirs/hetzell/code/protein_design/exampl...5Jul24_dna_design_0.2/openfold_monomer/alignments', use_single_seq_mode=False, output_dir='/nfs/homedirs/hetzell/code/protein_design/example_outputs/25Jul24_dna_design_0.2/openfold_monomer', model_device='cuda:0', config_preset='model_1', jax_param_path='modules/openfold/openfold/resources/params/params_model_1.npz', openfold_checkpoint_path=None, save_outputs=False, cpus=4, preset='full_dbs', output_postfix=None, data_random_seed=None, skip_relaxation=False, multimer_ri_gap=200, trace_model=False, subtract_plddt=False, long_sequence_inference=False, cif_output=False)\u001b[0m\n\u001b[1;32m     83\u001b[0m args \u001b[38;5;241m=\u001b[39m argparse\u001b[38;5;241m.\u001b[39mNamespace(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Example of what main might do:\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m \u001b[43mrun_pretrained_openfold\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n        args \u001b[0;34m= Namespace(fasta_dir='/nfs/homedirs/hetzell/code/protein_design/example_outputs/25Jul24_dna_design_0.2/protein_mpnn/seqs_best/monomer', template_mmcif_dir='/ceph/hdd/shared/hetzel_alphafold_database/pdb_mmcif/mmcif_files', use_precomputed_alignments='/nfs/homedirs/hetzell/code/protein_design/example_outputs/25Jul24_dna_design_0.2/openfold_monomer/alignments', use_single_seq_mode=False, output_dir='/nfs/homedirs/hetzell/code/protein_design/example_outputs/25Jul24_dna_design_0.2/openfold_monomer', model_device='cuda:0', config_preset='model_1', jax_param_path='modules/openfold/openfold/resources/params/params_model_1.npz', openfold_checkpoint_path=None, save_outputs=False, cpus=4, preset='full_dbs', output_postfix=None, data_random_seed=None, skip_relaxation=False, multimer_ri_gap=200, trace_model=False, subtract_plddt=False, long_sequence_inference=False, cif_output=False, uniref90_database_path=None, mgnify_database_path=None, pdb70_database_path=None, uniclust30_database_path=None, bfd_database_path=None, jackhmmer_binary_path='/usr/bin/jackhmmer', hhblits_binary_path='/usr/bin/hhblits', hhsearch_binary_path='/usr/bin/hhsearch', kalign_binary_path='/usr/bin/kalign', pdb_seqres_database_path=None, uniref30_database_path=None, uniprot_database_path=None, hmmsearch_binary_path='/usr/bin/hmmsearch', hmmbuild_binary_path='/usr/bin/hmmbuild', max_template_date='2024-07-25', obsolete_pdbs_path=None, release_dates_path=None)\u001b[0m\u001b[0;34m\n        \u001b[0mrun_pretrained_openfold \u001b[0;34m= <function main at 0x7f7929056ca0>\u001b[0m\n",
      "File \u001b[0;32m/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:288\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args=Namespace(fasta_dir='/nfs/homedirs/hetzell/code/...obsolete_pdbs_path=None, release_dates_path=None))\u001b[0m\n\u001b[1;32m    283\u001b[0m feature_dicts \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    284\u001b[0m model_generator \u001b[38;5;241m=\u001b[39m load_models_from_command_line(\n\u001b[1;32m    285\u001b[0m     config, args\u001b[38;5;241m.\u001b[39mmodel_device, args\u001b[38;5;241m.\u001b[39mopenfold_checkpoint_path, args\u001b[38;5;241m.\u001b[39mjax_param_path, args\u001b[38;5;241m.\u001b[39moutput_dir\n\u001b[1;32m    286\u001b[0m )\n\u001b[0;32m--> 288\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model, output_directory \u001b[38;5;129;01min\u001b[39;00m model_generator:\n        model_generator \u001b[0;34m= <generator object load_models_from_command_line at 0x7f79284ac740>\u001b[0m\n\u001b[1;32m    289\u001b[0m     cur_tracing_interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (tag, tags), seqs \u001b[38;5;129;01min\u001b[39;00m sorted_targets:\n",
      "File \u001b[0;32m/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/openfold/utils/script_utils.py:55\u001b[0m, in \u001b[0;36mload_models_from_command_line\u001b[0;34m(config=data:\n  common:\n    block_delete_msa:\n      msa_...erations: 20\n  stiffness: 10.0\n  tolerance: 2.39\n, model_device='cuda:0', openfold_checkpoint_path=None, jax_param_path='modules/openfold/openfold/resources/params/params_model_1.npz', output_dir='/nfs/homedirs/hetzell/code/protein_design/example_outputs/25Jul24_dna_design_0.2/openfold_monomer')\u001b[0m\n\u001b[1;32m     53\u001b[0m model_basename \u001b[38;5;241m=\u001b[39m get_model_basename(path)\n\u001b[1;32m     54\u001b[0m model_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(model_basename\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m:])\n\u001b[0;32m---> 55\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAlphaFold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n        config \u001b[0;34m= data:\n  common:\n    block_delete_msa:\n      msa_fraction_per_block: 0.3\n      num_blocks: 5\n      randomize_num_blocks: false\n    feat:\n      aatype:\n      - num residues placeholder\n      all_atom_mask:\n      - num residues placeholder\n      - null\n      all_atom_positions:\n      - num residues placeholder\n      - null\n      - null\n      alt_chi_angles:\n      - num residues placeholder\n      - null\n      atom14_alt_gt_exists:\n      - num residues placeholder\n      - null\n      atom14_alt_gt_positions:\n      - num residues placeholder\n      - null\n      - null\n      atom14_atom_exists:\n      - num residues placeholder\n      - null\n      atom14_atom_is_ambiguous:\n      - num residues placeholder\n      - null\n      atom14_gt_exists:\n      - num residues placeholder\n      - null\n      atom14_gt_positions:\n      - num residues placeholder\n      - null\n      - null\n      atom37_atom_exists:\n      - num residues placeholder\n      - null\n      backbone_rigid_mask:\n      - num residues placeholder\n      backbone_rigid_tensor:\n      - num residues placeholder\n      - null\n      - null\n      bert_mask:\n      - msa placeholder\n      - num residues placeholder\n      chi_angles_sin_cos:\n      - num residues placeholder\n      - null\n      - null\n      chi_mask:\n      - num residues placeholder\n      - null\n      extra_deletion_value:\n      - extra msa placeholder\n      - num residues placeholder\n      extra_has_deletion:\n      - extra msa placeholder\n      - num residues placeholder\n      extra_msa:\n      - extra msa placeholder\n      - num residues placeholder\n      extra_msa_mask:\n      - extra msa placeholder\n      - num residues placeholder\n      extra_msa_row_mask:\n      - extra msa placeholder\n      is_distillation: []\n      msa_feat:\n      - msa placeholder\n      - num residues placeholder\n      - null\n      msa_mask:\n      - msa placeholder\n      - num residues placeholder\n      msa_row_mask:\n      - msa placeholder\n      no_recycling_iters: []\n      pseudo_beta:\n      - num residues placeholder\n      - null\n      pseudo_beta_mask:\n      - num residues placeholder\n      residue_index:\n      - num residues placeholder\n      residx_atom14_to_atom37:\n      - num residues placeholder\n      - null\n      residx_atom37_to_atom14:\n      - num residues placeholder\n      - null\n      resolution: []\n      rigidgroups_alt_gt_frames:\n      - num residues placeholder\n      - null\n      - null\n      - null\n      rigidgroups_group_exists:\n      - num residues placeholder\n      - null\n      rigidgroups_group_is_ambiguous:\n      - num residues placeholder\n      - null\n      rigidgroups_gt_exists:\n      - num residues placeholder\n      - null\n      rigidgroups_gt_frames:\n      - num residues placeholder\n      - null\n      - null\n      - null\n      seq_length: []\n      seq_mask:\n      - num residues placeholder\n      target_feat:\n      - num residues placeholder\n      - null\n      template_aatype:\n      - num templates placeholder\n      - num residues placeholder\n      template_all_atom_mask:\n      - num templates placeholder\n      - num residues placeholder\n      - null\n      template_all_atom_positions:\n      - num templates placeholder\n      - num residues placeholder\n      - null\n      - null\n      template_alt_torsion_angles_sin_cos:\n      - num templates placeholder\n      - num residues placeholder\n      - null\n      - null\n      template_backbone_rigid_mask:\n      - num templates placeholder\n      - num residues placeholder\n      template_backbone_rigid_tensor:\n      - num templates placeholder\n      - num residues placeholder\n      - null\n      - null\n      template_mask:\n      - num templates placeholder\n      template_pseudo_beta:\n      - num templates placeholder\n      - num residues placeholder\n      - null\n      template_pseudo_beta_mask:\n      - num templates placeholder\n      - num residues placeholder\n      template_sum_probs:\n      - num templates placeholder\n      - null\n      template_torsion_angles_mask:\n      - num templates placeholder\n      - num residues placeholder\n      - null\n      template_torsion_angles_sin_cos:\n      - num templates placeholder\n      - num residues placeholder\n      - null\n      - null\n      true_msa:\n      - msa placeholder\n      - num residues placeholder\n      use_clamped_fape: []\n    masked_msa:\n      profile_prob: 0.1\n      same_prob: 0.1\n      uniform_prob: 0.1\n    max_recycling_iters: 3\n    msa_cluster_features: true\n    reduce_max_clusters_by_max_templates: true\n    reduce_msa_clusters_by_max_templates: false\n    resample_msa_in_recycling: true\n    template_features:\n    - template_all_atom_positions\n    - template_sum_probs\n    - template_aatype\n    - template_all_atom_mask\n    unsupervised_features:\n    - aatype\n    - residue_index\n    - msa\n    - num_alignments\n    - seq_length\n    - between_segment_residues\n    - deletion_matrix\n    - no_recycling_iters\n    use_template_torsion_angles: &id012 !!python/object:ml_collections.config_dict.config_dict.FieldReference\n      _field_type: &id001 !!python/name:builtins.bool ''\n      _ops: []\n      _required: false\n      _value: true\n    use_templates: &id013 !!python/object:ml_collections.config_dict.config_dict.FieldReference\n      _field_type: *id001\n      _ops: []\n      _required: false\n      _value: true\n  data_module:\n    data_loaders:\n      batch_size: 1\n      num_workers: 16\n      pin_memory: true\n    use_small_bfd: false\n  eval:\n    block_delete_msa: false\n    crop: false\n    crop_size: null\n    fixed_size: true\n    interface_threshold: null\n    masked_msa_replace_fraction: 0.15\n    max_extra_msa: 1024\n    max_msa_clusters: 128\n    max_template_hits: 4\n    max_templates: 4\n    spatial_crop_prob: null\n    subsample_templates: false\n    supervised: true\n    uniform_recycling: false\n  predict:\n    block_delete_msa: false\n    crop: false\n    crop_size: null\n    fixed_size: true\n    interface_threshold: null\n    masked_msa_replace_fraction: 0.15\n    max_extra_msa: 5120\n    max_msa_clusters: 512\n    max_template_hits: 4\n    max_templates: 4\n    spatial_crop_prob: null\n    subsample_templates: false\n    supervised: false\n    uniform_recycling: false\n  seqemb_mode:\n    enabled: false\n  supervised:\n    clamp_prob: 0.9\n    supervised_features:\n    - all_atom_mask\n    - all_atom_positions\n    - resolution\n    - use_clamped_fape\n    - is_distillation\n  train:\n    block_delete_msa: true\n    clamp_prob: 0.9\n    crop: true\n    crop_size: 256\n    distillation_prob: 0.75\n    fixed_size: true\n    interface_threshold: null\n    masked_msa_replace_fraction: 0.15\n    max_distillation_msa_clusters: 1000\n    max_extra_msa: 5120\n    max_msa_clusters: 128\n    max_template_hits: 4\n    max_templates: 4\n    shuffle_top_k_prefiltered: 20\n    spatial_crop_prob: 0.0\n    subsample_templates: true\n    supervised: true\n    uniform_recycling: true\nema:\n  decay: 0.999\nglobals:\n  blocks_per_ckpt: &id004 !!python/object:ml_collections.config_dict.config_dict.FieldReference\n    _field_type: &id002 !!python/name:builtins.int ''\n    _ops: []\n    _required: false\n    _value: null\n  c_e: &id008 !!python/object:ml_collections.config_dict.config_dict.FieldReference\n    _field_type: *id002\n    _ops: []\n    _required: false\n    _value: 64\n  c_m: &id005 !!python/object:ml_collections.config_dict.config_dict.FieldReference\n    _field_type: *id002\n    _ops: []\n    _required: false\n    _value: 256\n  c_s: &id006 !!python/object:ml_collections.config_dict.config_dict.FieldReference\n    _field_type: *id002\n    _ops: []\n    _required: false\n    _value: 384\n  c_t: &id014 !!python/object:ml_collections.config_dict.config_dict.FieldReference\n    _field_type: *id002\n    _ops: []\n    _required: false\n    _value: 64\n  c_z: &id007 !!python/object:ml_collections.config_dict.config_dict.FieldReference\n    _field_type: *id002\n    _ops: []\n    _required: false\n    _value: 128\n  chunk_size: !!python/object:ml_collections.config_dict.config_dict.FieldReference\n    _field_type: *id002\n    _ops: []\n    _required: false\n    _value: 4\n  eps: &id003 !!python/object:ml_collections.config_dict.config_dict.FieldReference\n    _field_type: !!python/name:builtins.float ''\n    _ops: []\n    _required: false\n    _value: 1.0e-08\n  is_multimer: false\n  offload_inference: false\n  seqemb_mode_enabled: false\n  use_deepspeed_evo_attention: false\n  use_flash: false\n  use_lma: false\nloss:\n  chain_center_of_mass:\n    clamp_distance: -4.0\n    enabled: false\n    eps: *id003\n    weight: 0.0\n  distogram:\n    eps: *id003\n    max_bin: 21.6875\n    min_bin: 2.3125\n    no_bins: 64\n    weight: 0.3\n  eps: *id003\n  experimentally_resolved:\n    eps: *id003\n    max_resolution: 3.0\n    min_resolution: 0.1\n    weight: 0.0\n  fape:\n    backbone:\n      clamp_distance: 10.0\n      loss_unit_distance: 10.0\n      weight: 0.5\n    eps: 0.0001\n    sidechain:\n      clamp_distance: 10.0\n      length_scale: 10.0\n      weight: 0.5\n    weight: 1.0\n  masked_msa:\n    eps: *id003\n    num_classes: 23\n    weight: 2.0\n  plddt_loss:\n    cutoff: 15.0\n    eps: *id003\n    max_resolution: 3.0\n    min_resolution: 0.1\n    no_bins: 50\n    weight: 0.01\n  supervised_chi:\n    angle_norm_weight: 0.01\n    chi_weight: 0.5\n    eps: *id003\n    weight: 1.0\n  tm:\n    enabled: &id010 !!python/object:ml_collections.config_dict.config_dict.FieldReference\n      _field_type: *id001\n      _ops: []\n      _required: false\n      _value: false\n    eps: *id003\n    max_bin: 31\n    max_resolution: 3.0\n    min_resolution: 0.1\n    no_bins: 64\n    weight: 0.0\n  violation:\n    average_clashes: false\n    clash_overlap_tolerance: 1.5\n    eps: *id003\n    violation_tolerance_factor: 12.0\n    weight: 0.0\nmodel:\n  _mask_trans: false\n  evoformer_stack:\n    blocks_per_ckpt: *id004\n    c_hidden_msa_att: 32\n    c_hidden_mul: 128\n    c_hidden_opm: 32\n    c_hidden_pair_att: 32\n    c_m: *id005\n    c_s: *id006\n    c_z: *id007\n    clear_cache_between_blocks: false\n    eps: *id003\n    fuse_projection_weights: false\n    inf: 1000000000.0\n    msa_dropout: 0.15\n    no_blocks: 48\n    no_column_attention: false\n    no_heads_msa: 8\n    no_heads_pair: 4\n    opm_first: false\n    pair_dropout: 0.25\n    transition_n: 4\n    tune_chunk_size: &id009 !!python/object:ml_collections.config_dict.config_dict.FieldReference\n      _field_type: *id001\n      _ops: []\n      _required: false\n      _value: true\n  extra_msa:\n    enabled: true\n    extra_msa_embedder:\n      c_in: 25\n      c_out: *id008\n    extra_msa_stack:\n      c_hidden_msa_att: 8\n      c_hidden_mul: 128\n      c_hidden_opm: 32\n      c_hidden_pair_att: 32\n      c_m: *id008\n      c_z: *id007\n      ckpt: true\n      clear_cache_between_blocks: false\n      eps: *id003\n      fuse_projection_weights: false\n      inf: 1000000000.0\n      msa_dropout: 0.15\n      no_blocks: 4\n      no_heads_msa: 8\n      no_heads_pair: 4\n      opm_first: false\n      pair_dropout: 0.25\n      transition_n: 4\n      tune_chunk_size: *id009\n  heads:\n    distogram:\n      c_z: *id007\n      no_bins: &id011 !!python/object:ml_collections.config_dict.config_dict.FieldReference\n        _field_type: *id002\n        _ops: []\n        _required: false\n        _value: 64\n    experimentally_resolved:\n      c_out: 37\n      c_s: *id006\n    lddt:\n      c_hidden: 128\n      c_in: *id006\n      no_bins: 50\n    masked_msa:\n      c_m: *id005\n      c_out: 23\n    tm:\n      c_z: *id007\n      enabled: *id010\n      no_bins: *id011\n  input_embedder:\n    c_m: *id005\n    c_z: *id007\n    msa_dim: 49\n    relpos_k: 32\n    tf_dim: 22\n  recycle_early_stop_tolerance: -1.0\n  recycling_embedder:\n    c_m: *id005\n    c_z: *id007\n    inf: 100000000.0\n    max_bin: 20.75\n    min_bin: 3.25\n    no_bins: 15\n  structure_module:\n    c_ipa: 16\n    c_resnet: 128\n    c_s: *id006\n    c_z: *id007\n    dropout_rate: 0.1\n    epsilon: *id003\n    inf: 100000.0\n    no_angles: 7\n    no_blocks: 8\n    no_heads_ipa: 12\n    no_qk_points: 4\n    no_resnet_blocks: 2\n    no_transition_layers: 1\n    no_v_points: 8\n    trans_scale_factor: 10\n  template:\n    average_templates: false\n    distogram:\n      max_bin: 50.75\n      min_bin: 3.25\n      no_bins: 39\n    embed_angles: *id012\n    enabled: *id013\n    eps: *id003\n    inf: 100000.0\n    offload_templates: false\n    template_pair_embedder:\n      c_in: 88\n      c_out: *id014\n    template_pair_stack:\n      blocks_per_ckpt: *id004\n      c_hidden_tri_att: 16\n      c_hidden_tri_mul: 64\n      c_t: *id014\n      dropout_rate: 0.25\n      fuse_projection_weights: false\n      inf: 1000000000.0\n      no_blocks: 2\n      no_heads: 4\n      pair_transition_n: 2\n      tri_mul_first: false\n      tune_chunk_size: *id009\n    template_pointwise_attention:\n      c_hidden: 16\n      c_t: *id014\n      c_z: *id007\n      inf: 100000.0\n      no_heads: 4\n    template_single_embedder:\n      c_in: 57\n      c_out: *id005\n    use_unit_vector: false\nrelax:\n  exclude_residues: []\n  max_iterations: 0\n  max_outer_iterations: 20\n  stiffness: 10.0\n  tolerance: 2.39\n\u001b[0m\n\u001b[1;32m     56\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     57\u001b[0m import_jax_weights_(model, path, version\u001b[38;5;241m=\u001b[39mmodel_version)\n",
      "File \u001b[0;32m/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/openfold/model/model.py:124\u001b[0m, in \u001b[0;36mAlphaFold.__init__\u001b[0;34m(self=AlphaFold(\n  (input_embedder): InputEmbedder(\n  ...gmoid()\n          )\n        )\n      )\n    )\n  )\n), config=data:\n  common:\n    block_delete_msa:\n      msa_...erations: 20\n  stiffness: 10.0\n  tolerance: 2.39\n)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextra_msa_embedder \u001b[38;5;241m=\u001b[39m ExtraMSAEmbedder(\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextra_msa_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_msa_embedder\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextra_msa_stack \u001b[38;5;241m=\u001b[39m ExtraMSAStack(\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextra_msa_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_msa_stack\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    122\u001b[0m     )\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevoformer \u001b[38;5;241m=\u001b[39m \u001b[43mEvoformerStack\u001b[49m\u001b[43m(\u001b[49m\n        self \u001b[0;34m= AlphaFold(\n  (input_embedder): InputEmbedder(\n    (linear_tf_z_i): Linear(in_features=22, out_features=128, bias=True)\n    (linear_tf_z_j): Linear(in_features=22, out_features=128, bias=True)\n    (linear_tf_m): Linear(in_features=22, out_features=256, bias=True)\n    (linear_msa_m): Linear(in_features=49, out_features=256, bias=True)\n    (linear_relpos): Linear(in_features=65, out_features=128, bias=True)\n  )\n  (recycling_embedder): RecyclingEmbedder(\n    (linear): Linear(in_features=15, out_features=128, bias=True)\n    (layer_norm_m): LayerNorm()\n    (layer_norm_z): LayerNorm()\n  )\n  (template_embedder): TemplateEmbedder(\n    (template_single_embedder): TemplateSingleEmbedder(\n      (linear_1): Linear(in_features=57, out_features=256, bias=True)\n      (relu): ReLU()\n      (linear_2): Linear(in_features=256, out_features=256, bias=True)\n    )\n    (template_pair_embedder): TemplatePairEmbedder(\n      (linear): Linear(in_features=88, out_features=64, bias=True)\n    )\n    (template_pair_stack): TemplatePairStack(\n      (blocks): ModuleList(\n        (0): TemplatePairStackBlock(\n          (dropout_row): DropoutRowwise(\n            (dropout): Dropout(p=0.25, inplace=False)\n          )\n          (dropout_col): DropoutColumnwise(\n            (dropout): Dropout(p=0.25, inplace=False)\n          )\n          (tri_att_start): TriangleAttention(\n            (layer_norm): LayerNorm()\n            (linear): Linear(in_features=64, out_features=4, bias=False)\n            (mha): Attention(\n              (linear_q): Linear(in_features=64, out_features=64, bias=False)\n              (linear_k): Linear(in_features=64, out_features=64, bias=False)\n              (linear_v): Linear(in_features=64, out_features=64, bias=False)\n              (linear_o): Linear(in_features=64, out_features=64, bias=True)\n              (linear_g): Linear(in_features=64, out_features=64, bias=True)\n              (sigmoid): Sigmoid()\n            )\n          )\n          (tri_att_end): TriangleAttentionEndingNode(\n            (layer_norm): LayerNorm()\n            (linear): Linear(in_features=64, out_features=4, bias=False)\n            (mha): Attention(\n              (linear_q): Linear(in_features=64, out_features=64, bias=False)\n              (linear_k): Linear(in_features=64, out_features=64, bias=False)\n              (linear_v): Linear(in_features=64, out_features=64, bias=False)\n              (linear_o): Linear(in_features=64, out_features=64, bias=True)\n              (linear_g): Linear(in_features=64, out_features=64, bias=True)\n              (sigmoid): Sigmoid()\n            )\n          )\n          (tri_mul_out): TriangleMultiplicationOutgoing(\n            (linear_g): Linear(in_features=64, out_features=64, bias=True)\n            (linear_z): Linear(in_features=64, out_features=64, bias=True)\n            (layer_norm_in): LayerNorm()\n            (layer_norm_out): LayerNorm()\n            (sigmoid): Sigmoid()\n            (linear_a_p): Linear(in_features=64, out_features=64, bias=True)\n            (linear_a_g): Linear(in_features=64, out_features=64, bias=True)\n            (linear_b_p): Linear(in_features=64, out_features=64, bias=True)\n            (linear_b_g): Linear(in_features=64, out_features=64, bias=True)\n          )\n          (tri_mul_in): TriangleMultiplicationIncoming(\n            (linear_g): Linear(in_features=64, out_features=64, bias=True)\n            (linear_z): Linear(in_features=64, out_features=64, bias=True)\n            (layer_norm_in): LayerNorm()\n            (layer_norm_out): LayerNorm()\n            (sigmoid): Sigmoid()\n            (linear_a_p): Linear(in_features=64, out_features=64, bias=True)\n            (linear_a_g): Linear(in_features=64, out_features=64, bias=True)\n            (linear_b_p): Linear(in_features=64, out_features=64, bias=True)\n            (linear_b_g): Linear(in_features=64, out_features=64, bias=True)\n          )\n          (pair_transition): PairTransition(\n            (layer_norm): LayerNorm()\n            (linear_1): Linear(in_features=64, out_features=128, bias=True)\n            (relu): ReLU()\n            (linear_2): Linear(in_features=128, out_features=64, bias=True)\n          )\n        )\n        (1): TemplatePairStackBlock(\n          (dropout_row): DropoutRowwise(\n            (dropout): Dropout(p=0.25, inplace=False)\n          )\n          (dropout_col): DropoutColumnwise(\n            (dropout): Dropout(p=0.25, inplace=False)\n          )\n          (tri_att_start): TriangleAttention(\n            (layer_norm): LayerNorm()\n            (linear): Linear(in_features=64, out_features=4, bias=False)\n            (mha): Attention(\n              (linear_q): Linear(in_features=64, out_features=64, bias=False)\n              (linear_k): Linear(in_features=64, out_features=64, bias=False)\n              (linear_v): Linear(in_features=64, out_features=64, bias=False)\n              (linear_o): Linear(in_features=64, out_features=64, bias=True)\n              (linear_g): Linear(in_features=64, out_features=64, bias=True)\n              (sigmoid): Sigmoid()\n            )\n          )\n          (tri_att_end): TriangleAttentionEndingNode(\n            (layer_norm): LayerNorm()\n            (linear): Linear(in_features=64, out_features=4, bias=False)\n            (mha): Attention(\n              (linear_q): Linear(in_features=64, out_features=64, bias=False)\n              (linear_k): Linear(in_features=64, out_features=64, bias=False)\n              (linear_v): Linear(in_features=64, out_features=64, bias=False)\n              (linear_o): Linear(in_features=64, out_features=64, bias=True)\n              (linear_g): Linear(in_features=64, out_features=64, bias=True)\n              (sigmoid): Sigmoid()\n            )\n          )\n          (tri_mul_out): TriangleMultiplicationOutgoing(\n            (linear_g): Linear(in_features=64, out_features=64, bias=True)\n            (linear_z): Linear(in_features=64, out_features=64, bias=True)\n            (layer_norm_in): LayerNorm()\n            (layer_norm_out): LayerNorm()\n            (sigmoid): Sigmoid()\n            (linear_a_p): Linear(in_features=64, out_features=64, bias=True)\n            (linear_a_g): Linear(in_features=64, out_features=64, bias=True)\n            (linear_b_p): Linear(in_features=64, out_features=64, bias=True)\n            (linear_b_g): Linear(in_features=64, out_features=64, bias=True)\n          )\n          (tri_mul_in): TriangleMultiplicationIncoming(\n            (linear_g): Linear(in_features=64, out_features=64, bias=True)\n            (linear_z): Linear(in_features=64, out_features=64, bias=True)\n            (layer_norm_in): LayerNorm()\n            (layer_norm_out): LayerNorm()\n            (sigmoid): Sigmoid()\n            (linear_a_p): Linear(in_features=64, out_features=64, bias=True)\n            (linear_a_g): Linear(in_features=64, out_features=64, bias=True)\n            (linear_b_p): Linear(in_features=64, out_features=64, bias=True)\n            (linear_b_g): Linear(in_features=64, out_features=64, bias=True)\n          )\n          (pair_transition): PairTransition(\n            (layer_norm): LayerNorm()\n            (linear_1): Linear(in_features=64, out_features=128, bias=True)\n            (relu): ReLU()\n            (linear_2): Linear(in_features=128, out_features=64, bias=True)\n          )\n        )\n      )\n      (layer_norm): LayerNorm()\n    )\n    (template_pointwise_att): TemplatePointwiseAttention(\n      (mha): Attention(\n        (linear_q): Linear(in_features=128, out_features=64, bias=False)\n        (linear_k): Linear(in_features=64, out_features=64, bias=False)\n        (linear_v): Linear(in_features=64, out_features=64, bias=False)\n        (linear_o): Linear(in_features=64, out_features=128, bias=True)\n        (sigmoid): Sigmoid()\n      )\n    )\n  )\n  (extra_msa_embedder): ExtraMSAEmbedder(\n    (linear): Linear(in_features=25, out_features=64, bias=True)\n  )\n  (extra_msa_stack): ExtraMSAStack(\n    (blocks): ModuleList(\n      (0): ExtraMSABlock(\n        (msa_att_row): MSARowAttentionWithPairBias(\n          (layer_norm_m): LayerNorm()\n          (layer_norm_z): LayerNorm()\n          (linear_z): Linear(in_features=128, out_features=8, bias=False)\n          (mha): Attention(\n            (linear_q): Linear(in_features=64, out_features=64, bias=False)\n            (linear_k): Linear(in_features=64, out_features=64, bias=False)\n            (linear_v): Linear(in_features=64, out_features=64, bias=False)\n            (linear_o): Linear(in_features=64, out_features=64, bias=True)\n            (linear_g): Linear(in_features=64, out_features=64, bias=True)\n            (sigmoid): Sigmoid()\n          )\n        )\n        (msa_dropout_layer): DropoutRowwise(\n          (dropout): Dropout(p=0.15, inplace=False)\n        )\n        (msa_transition): MSATransition(\n          (layer_norm): LayerNorm()\n          (linear_1): Linear(in_features=64, out_features=256, bias=True)\n          (relu): ReLU()\n          (linear_2): Linear(in_features=256, out_features=64, bias=True)\n        )\n        (outer_product_mean): OuterProductMean(\n          (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n          (linear_1): Linear(in_features=64, out_features=32, bias=True)\n          (linear_2): Linear(in_features=64, out_features=32, bias=True)\n          (linear_out): Linear(in_features=1024, out_features=128, bias=True)\n        )\n        (pair_stack): PairStack(\n          (tri_mul_out): TriangleMultiplicationOutgoing(\n            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n            (layer_norm_in): LayerNorm()\n            (layer_norm_out): LayerNorm()\n            (sigmoid): Sigmoid()\n            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n          )\n          (tri_mul_in): TriangleMultiplicationIncoming(\n            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n            (layer_norm_in): LayerNorm()\n            (layer_norm_out): LayerNorm()\n            (sigmoid): Sigmoid()\n            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n          )\n          (tri_att_start): TriangleAttention(\n            (layer_norm): LayerNorm()\n            (linear): Linear(in_features=128, out_features=4, bias=False)\n            (mha): Attention(\n              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n              (sigmoid): Sigmoid()\n            )\n          )\n          (tri_att_end): TriangleAttention(\n            (layer_norm): LayerNorm()\n            (linear): Linear(in_features=128, out_features=4, bias=False)\n            (mha): Attention(\n              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n              (sigmoid): Sigmoid()\n            )\n          )\n          (pair_transition): PairTransition(\n            (layer_norm): LayerNorm()\n            (linear_1): Linear(in_features=128, out_features=512, bias=True)\n            (relu): ReLU()\n            (linear_2): Linear(in_features=512, out_features=128, bias=True)\n          )\n          (ps_dropout_row_layer): DropoutRowwise(\n            (dropout): Dropout(p=0.25, inplace=False)\n          )\n        )\n        (msa_att_col): MSAColumnGlobalAttention(\n          (layer_norm_m): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n          (global_attention): GlobalAttention(\n            (linear_q): Linear(in_features=64, out_features=64, bias=False)\n            (linear_k): Linear(in_features=64, out_features=8, bias=False)\n            (linear_v): Linear(in_features=64, out_features=8, bias=False)\n            (linear_g): Linear(in_features=64, out_features=64, bias=True)\n            (linear_o): Linear(in_features=64, out_features=64, bias=True)\n            (sigmoid): Sigmoid()\n          )\n        )\n      )\n      (1): ExtraMSABlock(\n        (msa_att_row): MSARowAttentionWithPairBias(\n          (layer_norm_m): LayerNorm()\n          (layer_norm_z): LayerNorm()\n          (linear_z): Linear(in_features=128, out_features=8, bias=False)\n          (mha): Attention(\n            (linear_q): Linear(in_features=64, out_features=64, bias=False)\n            (linear_k): Linear(in_features=64, out_features=64, bias=False)\n            (linear_v): Linear(in_features=64, out_features=64, bias=False)\n            (linear_o): Linear(in_features=64, out_features=64, bias=True)\n            (linear_g): Linear(in_features=64, out_features=64, bias=True)\n            (sigmoid): Sigmoid()\n          )\n        )\n        (msa_dropout_layer): DropoutRowwise(\n          (dropout): Dropout(p=0.15, inplace=False)\n        )\n        (msa_transition): MSATransition(\n          (layer_norm): LayerNorm()\n          (linear_1): Linear(in_features=64, out_features=256, bias=True)\n          (relu): ReLU()\n          (linear_2): Linear(in_features=256, out_features=64, bias=True)\n        )\n        (outer_product_mean): OuterProductMean(\n          (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n          (linear_1): Linear(in_features=64, out_features=32, bias=True)\n          (linear_2): Linear(in_features=64, out_features=32, bias=True)\n          (linear_out): Linear(in_features=1024, out_features=128, bias=True)\n        )\n        (pair_stack): PairStack(\n          (tri_mul_out): TriangleMultiplicationOutgoing(\n            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n            (layer_norm_in): LayerNorm()\n            (layer_norm_out): LayerNorm()\n            (sigmoid): Sigmoid()\n            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n          )\n          (tri_mul_in): TriangleMultiplicationIncoming(\n            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n            (layer_norm_in): LayerNorm()\n            (layer_norm_out): LayerNorm()\n            (sigmoid): Sigmoid()\n            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n          )\n          (tri_att_start): TriangleAttention(\n            (layer_norm): LayerNorm()\n            (linear): Linear(in_features=128, out_features=4, bias=False)\n            (mha): Attention(\n              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n              (sigmoid): Sigmoid()\n            )\n          )\n          (tri_att_end): TriangleAttention(\n            (layer_norm): LayerNorm()\n            (linear): Linear(in_features=128, out_features=4, bias=False)\n            (mha): Attention(\n              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n              (sigmoid): Sigmoid()\n            )\n          )\n          (pair_transition): PairTransition(\n            (layer_norm): LayerNorm()\n            (linear_1): Linear(in_features=128, out_features=512, bias=True)\n            (relu): ReLU()\n            (linear_2): Linear(in_features=512, out_features=128, bias=True)\n          )\n          (ps_dropout_row_layer): DropoutRowwise(\n            (dropout): Dropout(p=0.25, inplace=False)\n          )\n        )\n        (msa_att_col): MSAColumnGlobalAttention(\n          (layer_norm_m): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n          (global_attention): GlobalAttention(\n            (linear_q): Linear(in_features=64, out_features=64, bias=False)\n            (linear_k): Linear(in_features=64, out_features=8, bias=False)\n            (linear_v): Linear(in_features=64, out_features=8, bias=False)\n            (linear_g): Linear(in_features=64, out_features=64, bias=True)\n            (linear_o): Linear(in_features=64, out_features=64, bias=True)\n            (sigmoid): Sigmoid()\n          )\n        )\n      )\n      (2): ExtraMSABlock(\n        (msa_att_row): MSARowAttentionWithPairBias(\n          (layer_norm_m): LayerNorm()\n          (layer_norm_z): LayerNorm()\n          (linear_z): Linear(in_features=128, out_features=8, bias=False)\n          (mha): Attention(\n            (linear_q): Linear(in_features=64, out_features=64, bias=False)\n            (linear_k): Linear(in_features=64, out_features=64, bias=False)\n            (linear_v): Linear(in_features=64, out_features=64, bias=False)\n            (linear_o): Linear(in_features=64, out_features=64, bias=True)\n            (linear_g): Linear(in_features=64, out_features=64, bias=True)\n            (sigmoid): Sigmoid()\n          )\n        )\n        (msa_dropout_layer): DropoutRowwise(\n          (dropout): Dropout(p=0.15, inplace=False)\n        )\n        (msa_transition): MSATransition(\n          (layer_norm): LayerNorm()\n          (linear_1): Linear(in_features=64, out_features=256, bias=True)\n          (relu): ReLU()\n          (linear_2): Linear(in_features=256, out_features=64, bias=True)\n        )\n        (outer_product_mean): OuterProductMean(\n          (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n          (linear_1): Linear(in_features=64, out_features=32, bias=True)\n          (linear_2): Linear(in_features=64, out_features=32, bias=True)\n          (linear_out): Linear(in_features=1024, out_features=128, bias=True)\n        )\n        (pair_stack): PairStack(\n          (tri_mul_out): TriangleMultiplicationOutgoing(\n            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n            (layer_norm_in): LayerNorm()\n            (layer_norm_out): LayerNorm()\n            (sigmoid): Sigmoid()\n            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n          )\n          (tri_mul_in): TriangleMultiplicationIncoming(\n            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n            (layer_norm_in): LayerNorm()\n            (layer_norm_out): LayerNorm()\n            (sigmoid): Sigmoid()\n            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n          )\n          (tri_att_start): TriangleAttention(\n            (layer_norm): LayerNorm()\n            (linear): Linear(in_features=128, out_features=4, bias=False)\n            (mha): Attention(\n              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n              (sigmoid): Sigmoid()\n            )\n          )\n          (tri_att_end): TriangleAttention(\n            (layer_norm): LayerNorm()\n            (linear): Linear(in_features=128, out_features=4, bias=False)\n            (mha): Attention(\n              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n              (sigmoid): Sigmoid()\n            )\n          )\n          (pair_transition): PairTransition(\n            (layer_norm): LayerNorm()\n            (linear_1): Linear(in_features=128, out_features=512, bias=True)\n            (relu): ReLU()\n            (linear_2): Linear(in_features=512, out_features=128, bias=True)\n          )\n          (ps_dropout_row_layer): DropoutRowwise(\n            (dropout): Dropout(p=0.25, inplace=False)\n          )\n        )\n        (msa_att_col): MSAColumnGlobalAttention(\n          (layer_norm_m): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n          (global_attention): GlobalAttention(\n            (linear_q): Linear(in_features=64, out_features=64, bias=False)\n            (linear_k): Linear(in_features=64, out_features=8, bias=False)\n            (linear_v): Linear(in_features=64, out_features=8, bias=False)\n            (linear_g): Linear(in_features=64, out_features=64, bias=True)\n            (linear_o): Linear(in_features=64, out_features=64, bias=True)\n            (sigmoid): Sigmoid()\n          )\n        )\n      )\n      (3): ExtraMSABlock(\n        (msa_att_row): MSARowAttentionWithPairBias(\n          (layer_norm_m): LayerNorm()\n          (layer_norm_z): LayerNorm()\n          (linear_z): Linear(in_features=128, out_features=8, bias=False)\n          (mha): Attention(\n            (linear_q): Linear(in_features=64, out_features=64, bias=False)\n            (linear_k): Linear(in_features=64, out_features=64, bias=False)\n            (linear_v): Linear(in_features=64, out_features=64, bias=False)\n            (linear_o): Linear(in_features=64, out_features=64, bias=True)\n            (linear_g): Linear(in_features=64, out_features=64, bias=True)\n            (sigmoid): Sigmoid()\n          )\n        )\n        (msa_dropout_layer): DropoutRowwise(\n          (dropout): Dropout(p=0.15, inplace=False)\n        )\n        (msa_transition): MSATransition(\n          (layer_norm): LayerNorm()\n          (linear_1): Linear(in_features=64, out_features=256, bias=True)\n          (relu): ReLU()\n          (linear_2): Linear(in_features=256, out_features=64, bias=True)\n        )\n        (outer_product_mean): OuterProductMean(\n          (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n          (linear_1): Linear(in_features=64, out_features=32, bias=True)\n          (linear_2): Linear(in_features=64, out_features=32, bias=True)\n          (linear_out): Linear(in_features=1024, out_features=128, bias=True)\n        )\n        (pair_stack): PairStack(\n          (tri_mul_out): TriangleMultiplicationOutgoing(\n            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n            (layer_norm_in): LayerNorm()\n            (layer_norm_out): LayerNorm()\n            (sigmoid): Sigmoid()\n            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n          )\n          (tri_mul_in): TriangleMultiplicationIncoming(\n            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n            (linear_z): Linear(in_features=128, out_features=128, bias=True)\n            (layer_norm_in): LayerNorm()\n            (layer_norm_out): LayerNorm()\n            (sigmoid): Sigmoid()\n            (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n            (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n            (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n            (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n          )\n          (tri_att_start): TriangleAttention(\n            (layer_norm): LayerNorm()\n            (linear): Linear(in_features=128, out_features=4, bias=False)\n            (mha): Attention(\n              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n              (sigmoid): Sigmoid()\n            )\n          )\n          (tri_att_end): TriangleAttention(\n            (layer_norm): LayerNorm()\n            (linear): Linear(in_features=128, out_features=4, bias=False)\n            (mha): Attention(\n              (linear_q): Linear(in_features=128, out_features=128, bias=False)\n              (linear_k): Linear(in_features=128, out_features=128, bias=False)\n              (linear_v): Linear(in_features=128, out_features=128, bias=False)\n              (linear_o): Linear(in_features=128, out_features=128, bias=True)\n              (linear_g): Linear(in_features=128, out_features=128, bias=True)\n              (sigmoid): Sigmoid()\n            )\n          )\n          (pair_transition): PairTransition(\n            (layer_norm): LayerNorm()\n            (linear_1): Linear(in_features=128, out_features=512, bias=True)\n            (relu): ReLU()\n            (linear_2): Linear(in_features=512, out_features=128, bias=True)\n          )\n          (ps_dropout_row_layer): DropoutRowwise(\n            (dropout): Dropout(p=0.25, inplace=False)\n          )\n        )\n        (msa_att_col): MSAColumnGlobalAttention(\n          (layer_norm_m): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n          (global_attention): GlobalAttention(\n            (linear_q): Linear(in_features=64, out_features=64, bias=False)\n            (linear_k): Linear(in_features=64, out_features=8, bias=False)\n            (linear_v): Linear(in_features=64, out_features=8, bias=False)\n            (linear_g): Linear(in_features=64, out_features=64, bias=True)\n            (linear_o): Linear(in_features=64, out_features=64, bias=True)\n            (sigmoid): Sigmoid()\n          )\n        )\n      )\n    )\n  )\n)\u001b[0m\u001b[0;34m\n        \u001b[0mself.config \u001b[0;34m= _mask_trans: false\nevoformer_stack:\n  blocks_per_ckpt: &id010 !!python/object:ml_collections.config_dict.config_dict.FieldReference\n    _field_type: &id001 !!python/name:builtins.int ''\n    _ops: []\n    _required: false\n    _value: null\n  c_hidden_msa_att: 32\n  c_hidden_mul: 128\n  c_hidden_opm: 32\n  c_hidden_pair_att: 32\n  c_m: &id007 !!python/object:ml_collections.config_dict.config_dict.FieldReference\n    _field_type: *id001\n    _ops: []\n    _required: false\n    _value: 256\n  c_s: &id006 !!python/object:ml_collections.config_dict.config_dict.FieldReference\n    _field_type: *id001\n    _ops: []\n    _required: false\n    _value: 384\n  c_z: &id003 !!python/object:ml_collections.config_dict.config_dict.FieldReference\n    _field_type: *id001\n    _ops: []\n    _required: false\n    _value: 128\n  clear_cache_between_blocks: false\n  eps: &id004 !!python/object:ml_collections.config_dict.config_dict.FieldReference\n    _field_type: !!python/name:builtins.float ''\n    _ops: []\n    _required: false\n    _value: 1.0e-08\n  fuse_projection_weights: false\n  inf: 1000000000.0\n  msa_dropout: 0.15\n  no_blocks: 48\n  no_column_attention: false\n  no_heads_msa: 8\n  no_heads_pair: 4\n  opm_first: false\n  pair_dropout: 0.25\n  transition_n: 4\n  tune_chunk_size: &id005 !!python/object:ml_collections.config_dict.config_dict.FieldReference\n    _field_type: &id008 !!python/name:builtins.bool ''\n    _ops: []\n    _required: false\n    _value: true\nextra_msa:\n  enabled: true\n  extra_msa_embedder:\n    c_in: 25\n    c_out: &id002 !!python/object:ml_collections.config_dict.config_dict.FieldReference\n      _field_type: *id001\n      _ops: []\n      _required: false\n      _value: 64\n  extra_msa_stack:\n    c_hidden_msa_att: 8\n    c_hidden_mul: 128\n    c_hidden_opm: 32\n    c_hidden_pair_att: 32\n    c_m: *id002\n    c_z: *id003\n    ckpt: true\n    clear_cache_between_blocks: false\n    eps: *id004\n    fuse_projection_weights: false\n    inf: 1000000000.0\n    msa_dropout: 0.15\n    no_blocks: 4\n    no_heads_msa: 8\n    no_heads_pair: 4\n    opm_first: false\n    pair_dropout: 0.25\n    transition_n: 4\n    tune_chunk_size: *id005\nheads:\n  distogram:\n    c_z: *id003\n    no_bins: &id009 !!python/object:ml_collections.config_dict.config_dict.FieldReference\n      _field_type: *id001\n      _ops: []\n      _required: false\n      _value: 64\n  experimentally_resolved:\n    c_out: 37\n    c_s: *id006\n  lddt:\n    c_hidden: 128\n    c_in: *id006\n    no_bins: 50\n  masked_msa:\n    c_m: *id007\n    c_out: 23\n  tm:\n    c_z: *id003\n    enabled: !!python/object:ml_collections.config_dict.config_dict.FieldReference\n      _field_type: *id008\n      _ops: []\n      _required: false\n      _value: false\n    no_bins: *id009\ninput_embedder:\n  c_m: *id007\n  c_z: *id003\n  msa_dim: 49\n  relpos_k: 32\n  tf_dim: 22\nrecycle_early_stop_tolerance: -1.0\nrecycling_embedder:\n  c_m: *id007\n  c_z: *id003\n  inf: 100000000.0\n  max_bin: 20.75\n  min_bin: 3.25\n  no_bins: 15\nstructure_module:\n  c_ipa: 16\n  c_resnet: 128\n  c_s: *id006\n  c_z: *id003\n  dropout_rate: 0.1\n  epsilon: *id004\n  inf: 100000.0\n  no_angles: 7\n  no_blocks: 8\n  no_heads_ipa: 12\n  no_qk_points: 4\n  no_resnet_blocks: 2\n  no_transition_layers: 1\n  no_v_points: 8\n  trans_scale_factor: 10\ntemplate:\n  average_templates: false\n  distogram:\n    max_bin: 50.75\n    min_bin: 3.25\n    no_bins: 39\n  embed_angles: !!python/object:ml_collections.config_dict.config_dict.FieldReference\n    _field_type: *id008\n    _ops: []\n    _required: false\n    _value: true\n  enabled: !!python/object:ml_collections.config_dict.config_dict.FieldReference\n    _field_type: *id008\n    _ops: []\n    _required: false\n    _value: true\n  eps: *id004\n  inf: 100000.0\n  offload_templates: false\n  template_pair_embedder:\n    c_in: 88\n    c_out: &id011 !!python/object:ml_collections.config_dict.config_dict.FieldReference\n      _field_type: *id001\n      _ops: []\n      _required: false\n      _value: 64\n  template_pair_stack:\n    blocks_per_ckpt: *id010\n    c_hidden_tri_att: 16\n    c_hidden_tri_mul: 64\n    c_t: *id011\n    dropout_rate: 0.25\n    fuse_projection_weights: false\n    inf: 1000000000.0\n    no_blocks: 2\n    no_heads: 4\n    pair_transition_n: 2\n    tri_mul_first: false\n    tune_chunk_size: *id005\n  template_pointwise_attention:\n    c_hidden: 16\n    c_t: *id011\n    c_z: *id003\n    inf: 100000.0\n    no_heads: 4\n  template_single_embedder:\n    c_in: 57\n    c_out: *id007\n  use_unit_vector: false\n\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mevoformer_stack\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstructure_module \u001b[38;5;241m=\u001b[39m StructureModule(\n\u001b[1;32m    129\u001b[0m     is_multimer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobals\u001b[38;5;241m.\u001b[39mis_multimer,\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstructure_module\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    131\u001b[0m )\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maux_heads \u001b[38;5;241m=\u001b[39m AuxiliaryHeads(\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheads\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    134\u001b[0m )\n",
      "File \u001b[0;32m/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/openfold/model/evoformer.py:835\u001b[0m, in \u001b[0;36mEvoformerStack.__init__\u001b[0;34m(self=EvoformerStack(\n  (blocks): ModuleList(\n    (0):...gmoid()\n          )\n        )\n      )\n    )\n  )\n), c_m=256, c_z=128, c_hidden_msa_att=32, c_hidden_opm=32, c_hidden_mul=128, c_hidden_pair_att=32, c_s=384, no_heads_msa=8, no_heads_pair=4, no_blocks=48, transition_n=4, msa_dropout=0.15, pair_dropout=0.25, no_column_attention=False, opm_first=False, fuse_projection_weights=False, blocks_per_ckpt=None, inf=1000000000.0, eps=1e-08, clear_cache_between_blocks=False, tune_chunk_size=True, **kwargs={})\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList()\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(no_blocks):\n\u001b[0;32m--> 835\u001b[0m     block \u001b[38;5;241m=\u001b[39m \u001b[43mEvoformerBlock\u001b[49m\u001b[43m(\u001b[49m\n        block \u001b[0;34m= EvoformerBlock(\n  (msa_att_row): MSARowAttentionWithPairBias(\n    (layer_norm_m): LayerNorm()\n    (layer_norm_z): LayerNorm()\n    (linear_z): Linear(in_features=128, out_features=8, bias=False)\n    (mha): Attention(\n      (linear_q): Linear(in_features=256, out_features=256, bias=False)\n      (linear_k): Linear(in_features=256, out_features=256, bias=False)\n      (linear_v): Linear(in_features=256, out_features=256, bias=False)\n      (linear_o): Linear(in_features=256, out_features=256, bias=True)\n      (linear_g): Linear(in_features=256, out_features=256, bias=True)\n      (sigmoid): Sigmoid()\n    )\n  )\n  (msa_dropout_layer): DropoutRowwise(\n    (dropout): Dropout(p=0.15, inplace=False)\n  )\n  (msa_transition): MSATransition(\n    (layer_norm): LayerNorm()\n    (linear_1): Linear(in_features=256, out_features=1024, bias=True)\n    (relu): ReLU()\n    (linear_2): Linear(in_features=1024, out_features=256, bias=True)\n  )\n  (outer_product_mean): OuterProductMean(\n    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n    (linear_1): Linear(in_features=256, out_features=32, bias=True)\n    (linear_2): Linear(in_features=256, out_features=32, bias=True)\n    (linear_out): Linear(in_features=1024, out_features=128, bias=True)\n  )\n  (pair_stack): PairStack(\n    (tri_mul_out): TriangleMultiplicationOutgoing(\n      (linear_g): Linear(in_features=128, out_features=128, bias=True)\n      (linear_z): Linear(in_features=128, out_features=128, bias=True)\n      (layer_norm_in): LayerNorm()\n      (layer_norm_out): LayerNorm()\n      (sigmoid): Sigmoid()\n      (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n      (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n      (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n      (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n    )\n    (tri_mul_in): TriangleMultiplicationIncoming(\n      (linear_g): Linear(in_features=128, out_features=128, bias=True)\n      (linear_z): Linear(in_features=128, out_features=128, bias=True)\n      (layer_norm_in): LayerNorm()\n      (layer_norm_out): LayerNorm()\n      (sigmoid): Sigmoid()\n      (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n      (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n      (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n      (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n    )\n    (tri_att_start): TriangleAttention(\n      (layer_norm): LayerNorm()\n      (linear): Linear(in_features=128, out_features=4, bias=False)\n      (mha): Attention(\n        (linear_q): Linear(in_features=128, out_features=128, bias=False)\n        (linear_k): Linear(in_features=128, out_features=128, bias=False)\n        (linear_v): Linear(in_features=128, out_features=128, bias=False)\n        (linear_o): Linear(in_features=128, out_features=128, bias=True)\n        (linear_g): Linear(in_features=128, out_features=128, bias=True)\n        (sigmoid): Sigmoid()\n      )\n    )\n    (tri_att_end): TriangleAttention(\n      (layer_norm): LayerNorm()\n      (linear): Linear(in_features=128, out_features=4, bias=False)\n      (mha): Attention(\n        (linear_q): Linear(in_features=128, out_features=128, bias=False)\n        (linear_k): Linear(in_features=128, out_features=128, bias=False)\n        (linear_v): Linear(in_features=128, out_features=128, bias=False)\n        (linear_o): Linear(in_features=128, out_features=128, bias=True)\n        (linear_g): Linear(in_features=128, out_features=128, bias=True)\n        (sigmoid): Sigmoid()\n      )\n    )\n    (pair_transition): PairTransition(\n      (layer_norm): LayerNorm()\n      (linear_1): Linear(in_features=128, out_features=512, bias=True)\n      (relu): ReLU()\n      (linear_2): Linear(in_features=512, out_features=128, bias=True)\n    )\n    (ps_dropout_row_layer): DropoutRowwise(\n      (dropout): Dropout(p=0.25, inplace=False)\n    )\n  )\n  (msa_att_col): MSAColumnAttention(\n    (_msa_att): MSAAttention(\n      (layer_norm_m): LayerNorm()\n      (mha): Attention(\n        (linear_q): Linear(in_features=256, out_features=256, bias=False)\n        (linear_k): Linear(in_features=256, out_features=256, bias=False)\n        (linear_v): Linear(in_features=256, out_features=256, bias=False)\n        (linear_o): Linear(in_features=256, out_features=256, bias=True)\n        (linear_g): Linear(in_features=256, out_features=256, bias=True)\n        (sigmoid): Sigmoid()\n      )\n    )\n  )\n)\u001b[0m\u001b[0;34m\n        \u001b[0mEvoformerBlock \u001b[0;34m= <class 'openfold.model.evoformer.EvoformerBlock'>\u001b[0m\u001b[0;34m\n        \u001b[0mc_m \u001b[0;34m= 256\u001b[0m\u001b[0;34m\n        \u001b[0mc_z \u001b[0;34m= 128\u001b[0m\u001b[0;34m\n        \u001b[0mc_hidden_msa_att \u001b[0;34m= 32\u001b[0m\u001b[0;34m\n        \u001b[0mc_hidden_opm \u001b[0;34m= 32\u001b[0m\u001b[0;34m\n        \u001b[0mc_hidden_mul \u001b[0;34m= 128\u001b[0m\u001b[0;34m\n        \u001b[0mc_hidden_pair_att \u001b[0;34m= 32\u001b[0m\u001b[0;34m\n        \u001b[0mno_heads_msa \u001b[0;34m= 8\u001b[0m\u001b[0;34m\n        \u001b[0mno_heads_pair \u001b[0;34m= 4\u001b[0m\u001b[0;34m\n        \u001b[0mtransition_n \u001b[0;34m= 4\u001b[0m\u001b[0;34m\n        \u001b[0mmsa_dropout \u001b[0;34m= 0.15\u001b[0m\u001b[0;34m\n        \u001b[0mpair_dropout \u001b[0;34m= 0.25\u001b[0m\u001b[0;34m\n        \u001b[0mno_column_attention \u001b[0;34m= False\u001b[0m\u001b[0;34m\n        \u001b[0mopm_first \u001b[0;34m= False\u001b[0m\u001b[0;34m\n        \u001b[0mfuse_projection_weights \u001b[0;34m= False\u001b[0m\u001b[0;34m\n        \u001b[0minf \u001b[0;34m= 1000000000.0\u001b[0m\u001b[0;34m\n        \u001b[0meps \u001b[0;34m= 1e-08\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[43m        \u001b[49m\u001b[43mc_m\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc_m\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m        \u001b[49m\u001b[43mc_z\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m        \u001b[49m\u001b[43mc_hidden_msa_att\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc_hidden_msa_att\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m        \u001b[49m\u001b[43mc_hidden_opm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc_hidden_opm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mc_hidden_mul\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc_hidden_mul\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mc_hidden_pair_att\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc_hidden_pair_att\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mno_heads_msa\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_heads_msa\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mno_heads_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_heads_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransition_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransition_n\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmsa_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsa_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpair_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpair_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mno_column_attention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_column_attention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopm_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopm_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfuse_projection_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuse_projection_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43minf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\u001b[38;5;241m.\u001b[39mappend(block)\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear \u001b[38;5;241m=\u001b[39m Linear(c_m, c_s)\n",
      "File \u001b[0;32m/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/openfold/model/evoformer.py:396\u001b[0m, in \u001b[0;36mEvoformerBlock.__init__\u001b[0;34m(self=EvoformerBlock(\n  (msa_att_row): MSARowAttention...features=1024, out_features=256, bias=True)\n  )\n), c_m=256, c_z=128, c_hidden_msa_att=32, c_hidden_opm=32, c_hidden_mul=128, c_hidden_pair_att=32, no_heads_msa=8, no_heads_pair=4, transition_n=4, msa_dropout=0.15, pair_dropout=0.25, no_column_attention=False, opm_first=False, fuse_projection_weights=False, inf=1000000000.0, eps=1e-08)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    379\u001b[0m     c_m: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m    380\u001b[0m     c_z: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m     eps: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[1;32m    395\u001b[0m ):\n\u001b[0;32m--> 396\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mEvoformerBlock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mc_m\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc_m\u001b[49m\u001b[43m,\u001b[49m\n        self \u001b[0;34m= EvoformerBlock(\n  (msa_att_row): MSARowAttentionWithPairBias(\n    (layer_norm_m): LayerNorm()\n    (layer_norm_z): LayerNorm()\n    (linear_z): Linear(in_features=128, out_features=8, bias=False)\n    (mha): Attention(\n      (linear_q): Linear(in_features=256, out_features=256, bias=False)\n      (linear_k): Linear(in_features=256, out_features=256, bias=False)\n      (linear_v): Linear(in_features=256, out_features=256, bias=False)\n      (linear_o): Linear(in_features=256, out_features=256, bias=True)\n      (linear_g): Linear(in_features=256, out_features=256, bias=True)\n      (sigmoid): Sigmoid()\n    )\n  )\n  (msa_dropout_layer): DropoutRowwise(\n    (dropout): Dropout(p=0.15, inplace=False)\n  )\n  (msa_transition): MSATransition(\n    (layer_norm): LayerNorm()\n    (linear_1): Linear(in_features=256, out_features=1024, bias=True)\n    (relu): ReLU()\n    (linear_2): Linear(in_features=1024, out_features=256, bias=True)\n  )\n)\u001b[0m\u001b[0;34m\n        \u001b[0mc_m \u001b[0;34m= 256\u001b[0m\u001b[0;34m\n        \u001b[0mEvoformerBlock \u001b[0;34m= <class 'openfold.model.evoformer.EvoformerBlock'>\u001b[0m\u001b[0;34m\n        \u001b[0mc_z \u001b[0;34m= 128\u001b[0m\u001b[0;34m\n        \u001b[0mc_hidden_msa_att \u001b[0;34m= 32\u001b[0m\u001b[0;34m\n        \u001b[0mc_hidden_opm \u001b[0;34m= 32\u001b[0m\u001b[0;34m\n        \u001b[0mc_hidden_mul \u001b[0;34m= 128\u001b[0m\u001b[0;34m\n        \u001b[0mc_hidden_pair_att \u001b[0;34m= 32\u001b[0m\u001b[0;34m\n        \u001b[0mno_heads_msa \u001b[0;34m= 8\u001b[0m\u001b[0;34m\n        \u001b[0mno_heads_pair \u001b[0;34m= 4\u001b[0m\u001b[0;34m\n        \u001b[0mtransition_n \u001b[0;34m= 4\u001b[0m\u001b[0;34m\n        \u001b[0mmsa_dropout \u001b[0;34m= 0.15\u001b[0m\u001b[0;34m\n        \u001b[0mpair_dropout \u001b[0;34m= 0.25\u001b[0m\u001b[0;34m\n        \u001b[0mopm_first \u001b[0;34m= False\u001b[0m\u001b[0;34m\n        \u001b[0mfuse_projection_weights \u001b[0;34m= False\u001b[0m\u001b[0;34m\n        \u001b[0minf \u001b[0;34m= 1000000000.0\u001b[0m\u001b[0;34m\n        \u001b[0meps \u001b[0;34m= 1e-08\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mc_z\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mc_hidden_msa_att\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc_hidden_msa_att\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mc_hidden_opm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc_hidden_opm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mc_hidden_mul\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc_hidden_mul\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mc_hidden_pair_att\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc_hidden_pair_att\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mno_heads_msa\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_heads_msa\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mno_heads_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_heads_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mtransition_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransition_n\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mmsa_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsa_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mpair_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpair_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mopm_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopm_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mfuse_projection_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuse_projection_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43minf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;66;03m# Specifically, seqemb mode does not use column attention\u001b[39;00m\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mno_column_attention \u001b[38;5;241m=\u001b[39m no_column_attention\n",
      "File \u001b[0;32m/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/openfold/model/evoformer.py:307\u001b[0m, in \u001b[0;36mMSABlock.__init__\u001b[0;34m(self=EvoformerBlock(\n  (msa_att_row): MSARowAttention...features=1024, out_features=256, bias=True)\n  )\n), c_m=256, c_z=128, c_hidden_msa_att=32, c_hidden_opm=32, c_hidden_mul=128, c_hidden_pair_att=32, no_heads_msa=8, no_heads_pair=4, transition_n=4, msa_dropout=0.15, pair_dropout=0.25, opm_first=False, fuse_projection_weights=False, inf=1000000000.0, eps=1e-08)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmsa_dropout_layer \u001b[38;5;241m=\u001b[39m DropoutRowwise(msa_dropout)\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmsa_transition \u001b[38;5;241m=\u001b[39m MSATransition(\n\u001b[1;32m    303\u001b[0m     c_m\u001b[38;5;241m=\u001b[39mc_m,\n\u001b[1;32m    304\u001b[0m     n\u001b[38;5;241m=\u001b[39mtransition_n,\n\u001b[1;32m    305\u001b[0m )\n\u001b[0;32m--> 307\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mouter_product_mean \u001b[38;5;241m=\u001b[39m \u001b[43mOuterProductMean\u001b[49m\u001b[43m(\u001b[49m\n        self \u001b[0;34m= EvoformerBlock(\n  (msa_att_row): MSARowAttentionWithPairBias(\n    (layer_norm_m): LayerNorm()\n    (layer_norm_z): LayerNorm()\n    (linear_z): Linear(in_features=128, out_features=8, bias=False)\n    (mha): Attention(\n      (linear_q): Linear(in_features=256, out_features=256, bias=False)\n      (linear_k): Linear(in_features=256, out_features=256, bias=False)\n      (linear_v): Linear(in_features=256, out_features=256, bias=False)\n      (linear_o): Linear(in_features=256, out_features=256, bias=True)\n      (linear_g): Linear(in_features=256, out_features=256, bias=True)\n      (sigmoid): Sigmoid()\n    )\n  )\n  (msa_dropout_layer): DropoutRowwise(\n    (dropout): Dropout(p=0.15, inplace=False)\n  )\n  (msa_transition): MSATransition(\n    (layer_norm): LayerNorm()\n    (linear_1): Linear(in_features=256, out_features=1024, bias=True)\n    (relu): ReLU()\n    (linear_2): Linear(in_features=1024, out_features=256, bias=True)\n  )\n)\u001b[0m\u001b[0;34m\n        \u001b[0mc_m \u001b[0;34m= 256\u001b[0m\u001b[0;34m\n        \u001b[0mc_z \u001b[0;34m= 128\u001b[0m\u001b[0;34m\n        \u001b[0mc_hidden_opm \u001b[0;34m= 32\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mc_m\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mc_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mc_hidden_opm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpair_stack \u001b[38;5;241m=\u001b[39m PairStack(\n\u001b[1;32m    314\u001b[0m     c_z\u001b[38;5;241m=\u001b[39mc_z,\n\u001b[1;32m    315\u001b[0m     c_hidden_mul\u001b[38;5;241m=\u001b[39mc_hidden_mul,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    322\u001b[0m     eps\u001b[38;5;241m=\u001b[39meps\n\u001b[1;32m    323\u001b[0m )\n",
      "File \u001b[0;32m/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/openfold/model/outer_product_mean.py:50\u001b[0m, in \u001b[0;36mOuterProductMean.__init__\u001b[0;34m(self=OuterProductMean(\n  (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n), c_m=256, c_z=128, c_hidden=32, eps=0.001)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps \u001b[38;5;241m=\u001b[39m eps\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm(c_m)\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_1 \u001b[38;5;241m=\u001b[39m \u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc_m\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_hidden\u001b[49m\u001b[43m)\u001b[49m\n        c_m \u001b[0;34m= 256\u001b[0m\u001b[0;34m\n        \u001b[0mc_hidden \u001b[0;34m= 32\u001b[0m\u001b[0;34m\n        \u001b[0mself \u001b[0;34m= OuterProductMean(\n  (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_2 \u001b[38;5;241m=\u001b[39m Linear(c_m, c_hidden)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_out \u001b[38;5;241m=\u001b[39m Linear(c_hidden \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, c_z, init\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/openfold/model/primitives.py:159\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self=Linear(in_features=256, out_features=32, bias=True), in_dim=256, out_dim=32, bias=True, init='default', init_fn=None, precision=None)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    129\u001b[0m     in_dim: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    134\u001b[0m     precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    135\u001b[0m ):\n\u001b[1;32m    136\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03m        in_dim:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m            Overrides init if not None.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mLinear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43min_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n        bias \u001b[0;34m= True\u001b[0m\u001b[0;34m\n        \u001b[0min_dim \u001b[0;34m= 256\u001b[0m\u001b[0;34m\n        \u001b[0mout_dim \u001b[0;34m= 32\u001b[0m\u001b[0;34m\n        \u001b[0mself \u001b[0;34m= Linear(in_features=256, out_features=32, bias=True)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m/nfs/staff-hdd/hetzell/miniconda3/envs/protein-design-env/lib/python3.9/site-packages/torch/nn/modules/linear.py:101\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self=Linear(in_features=256, out_features=32, bias=True), in_features=256, out_features=32, bias=True, device=None, dtype=None)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n        self \u001b[0;34m= Linear(in_features=256, out_features=32, bias=True)\u001b[0m\n",
      "File \u001b[0;32m/nfs/staff-hdd/hetzell/miniconda3/envs/protein-design-env/lib/python3.9/site-packages/torch/nn/modules/linear.py:107\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self=Linear(in_features=256, out_features=32, bias=True))\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     \u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkaiming_uniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n        init \u001b[0;34m= <module 'torch.nn.init' from '/nfs/staff-hdd/hetzell/miniconda3/envs/protein-design-env/lib/python3.9/site-packages/torch/nn/init.py'>\u001b[0m\u001b[0;34m\n        \u001b[0mself \u001b[0;34m= Linear(in_features=256, out_features=32, bias=True)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m         fan_in, _ \u001b[38;5;241m=\u001b[39m init\u001b[38;5;241m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n",
      "File \u001b[0;32m/nfs/staff-hdd/hetzell/miniconda3/envs/protein-design-env/lib/python3.9/site-packages/torch/nn/init.py:412\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor=Parameter containing:\ntensor([[ 0.0452,  0.0345,...8,  0.0253,  0.0172]],\n       requires_grad=True), a=2.23606797749979, mode='fan_in', nonlinearity='leaky_relu')\u001b[0m\n\u001b[1;32m    410\u001b[0m bound \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m3.0\u001b[39m) \u001b[38;5;241m*\u001b[39m std  \u001b[38;5;66;03m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbound\u001b[49m\u001b[43m)\u001b[49m\n        bound \u001b[0;34m= 0.06249999999999999\u001b[0m\u001b[0;34m\n        \u001b[0mtensor \u001b[0;34m= Parameter containing:\ntensor([[ 0.0452,  0.0345,  0.0118,  ...,  0.0440,  0.0567,  0.0441],\n        [-0.0031, -0.0280,  0.0605,  ..., -0.0607, -0.0391,  0.0135],\n        [-0.0265, -0.0031, -0.0470,  ..., -0.0017,  0.0128, -0.0435],\n        ...,\n        [-0.0133,  0.0304,  0.0314,  ..., -0.0014, -0.0428, -0.0244],\n        [ 0.0572, -0.0132, -0.0188,  ..., -0.0161, -0.0307, -0.0586],\n        [ 0.0199, -0.0350, -0.0013,  ...,  0.0228,  0.0253,  0.0172]],\n       requires_grad=True)\u001b[0m\u001b[0;34m\n        \u001b[0m-bound \u001b[0;34m= -0.06249999999999999\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # Only test on a subset of sequences\n",
    "# _FASTA_DIR = FASTA_DIR.parent / \"trimer_linker_test\"\n",
    "\n",
    "run_openfold(\n",
    "    fasta_dir=str(FASTA_DIR),\n",
    "    use_precomputed_alignments=str(OUTPUT_OPENFOLD / \"alignments\"),\n",
    "    config_preset=\"model_1\",\n",
    "    model_device=\"cuda:0\",\n",
    "    output_dir=str(OUTPUT_OPENFOLD),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting = \"homomer\"\n",
    "\n",
    "OUTPUT_OPENFOLD = OUTPUT_DIR / f\"openfold_{setting}\"\n",
    "# Only test on a subset of sequences\n",
    "_FASTA_DIR = OUTPUT_DIR / \"protein_mpnn\" / \"seqs_best\" / \"homomer\"\n",
    "\n",
    "create_empty_msas(str(_FASTA_DIR), str(OUTPUT_OPENFOLD / \"alignments\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/openfold/utils/script_utils.py:Successfully loaded JAX parameters at modules/openfold/openfold/resources/params/params_model_1_multimer_v3.npz...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_14_0__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_1_1__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_11_0__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_1_2__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_24_0__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_29_2__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_0_0__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_13_2__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_5_1__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_13_1__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_26_0__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_27_0__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_25_0__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_12_1__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_28_1__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_23_1__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_22_0__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_0_2__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_7_2__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_4_1__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_8_0__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_21_0__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_24_1__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_13_0__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_28_2__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_19_1__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_16_1__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_2_0__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_20_1__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_5_2__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_2_1__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_27_1__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_19_0__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_14_2__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_17_2__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_27_2__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_29_1__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_3_1__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_17_1__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_23_2__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_1_0__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_21_1__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_10_1__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_20_0__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_28_0__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_3_2__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_12_2__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_29_0__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_18_1__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_24_2__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_2_2__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_16_2__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_7_1__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_0_1__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_9_2__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_11_2__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_11_1__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_18_2__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_23_0__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_3_0__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_9_0__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_15_0__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_10_0__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_25_1__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_10_2__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_21_2__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_15_2__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_4_2__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_14_1__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_8_2__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_8_1__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_26_1__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_26_2__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_12_0__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_6_2__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_9_1__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_16_0__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_15_1__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_4_0__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_18_0__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_5_0__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_22_2__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_6_1__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_6_0__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_17_0__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_20_2__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_7_0__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_19_2__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_22_1__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n",
      "INFO:/nfs/staff-ssd/hetzell/code/protein_design/modules/openfold/run_pretrained_openfold.py:Skipping C8_oligo_25_2__A-B-C-D-E-F-G-H_model_1_multimer_v3 as it already exists...\n"
     ]
    }
   ],
   "source": [
    "# Only test on a subset of sequences\n",
    "_FASTA_DIR = OUTPUT_DIR / \"protein_mpnn\" / \"seqs_best\" / \"homomer\"\n",
    "\n",
    "run_openfold(\n",
    "    fasta_dir=str(_FASTA_DIR),\n",
    "    use_precomputed_alignments=str(OUTPUT_OPENFOLD / \"alignments\"),\n",
    "    config_preset=\"model_1_multimer_v3\",\n",
    "    model_device=\"cuda:0\",\n",
    "    output_dir=str(OUTPUT_OPENFOLD),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protein-design-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
